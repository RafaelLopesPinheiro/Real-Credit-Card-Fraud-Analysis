{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-06T14:02:00.028457Z","iopub.execute_input":"2022-04-06T14:02:00.028767Z","iopub.status.idle":"2022-04-06T14:02:00.045298Z","shell.execute_reply.started":"2022-04-06T14:02:00.028736Z","shell.execute_reply":"2022-04-06T14:02:00.044544Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"markdown","source":">  As nouns the difference between default and fraud is that default is (finance) the condition of failing to meet an obligation while fraud is any act of deception carried out for the purpose of unfair, undeserved and/or unlawful gain. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:00.092541Z","iopub.execute_input":"2022-04-06T14:02:00.093362Z","iopub.status.idle":"2022-04-06T14:02:00.098842Z","shell.execute_reply.started":"2022-04-06T14:02:00.093316Z","shell.execute_reply":"2022-04-06T14:02:00.097938Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/creditcardfraud/creditcard.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:00.158099Z","iopub.execute_input":"2022-04-06T14:02:00.158826Z","iopub.status.idle":"2022-04-06T14:02:03.512992Z","shell.execute_reply.started":"2022-04-06T14:02:00.158791Z","shell.execute_reply":"2022-04-06T14:02:03.512003Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:03.515332Z","iopub.execute_input":"2022-04-06T14:02:03.515898Z","iopub.status.idle":"2022-04-06T14:02:04.041984Z","shell.execute_reply.started":"2022-04-06T14:02:03.515846Z","shell.execute_reply":"2022-04-06T14:02:04.041294Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.043492Z","iopub.execute_input":"2022-04-06T14:02:04.044043Z","iopub.status.idle":"2022-04-06T14:02:04.073502Z","shell.execute_reply.started":"2022-04-06T14:02:04.043994Z","shell.execute_reply":"2022-04-06T14:02:04.072437Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y=df['Time'])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.075613Z","iopub.execute_input":"2022-04-06T14:02:04.076004Z","iopub.status.idle":"2022-04-06T14:02:04.274939Z","shell.execute_reply.started":"2022-04-06T14:02:04.075966Z","shell.execute_reply":"2022-04-06T14:02:04.274036Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df['Amount']); #has outliers","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.276567Z","iopub.execute_input":"2022-04-06T14:02:04.277272Z","iopub.status.idle":"2022-04-06T14:02:04.572738Z","shell.execute_reply.started":"2022-04-06T14:02:04.277219Z","shell.execute_reply":"2022-04-06T14:02:04.571218Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"print(round(df['Class'].value_counts()[0]/ len(df)*100,2))\nprint(round(df['Class'].value_counts()[1]/ len(df)*100,2))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.574358Z","iopub.execute_input":"2022-04-06T14:02:04.574605Z","iopub.status.idle":"2022-04-06T14:02:04.587048Z","shell.execute_reply.started":"2022-04-06T14:02:04.574575Z","shell.execute_reply":"2022-04-06T14:02:04.585950Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x='Class'); #heavely skewed","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.588670Z","iopub.execute_input":"2022-04-06T14:02:04.589006Z","iopub.status.idle":"2022-04-06T14:02:04.793544Z","shell.execute_reply.started":"2022-04-06T14:02:04.588959Z","shell.execute_reply":"2022-04-06T14:02:04.792496Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,6))\n\namount_values = df['Amount'].values\ntime_values = df['Time'].values\n\nsns.distplot(amount_values, ax=ax[0], color='r')\nax[0].set_xlim([min(amount_values), max(amount_values)])\nax[0].set_title('Distribution of Amounts', fontsize= 15)\n\nsns.distplot(time_values, ax=ax[1], color='g')\nax[1].set_xlim([min(time_values), max(time_values)])\nax[1].set_title('Distribution of Time', fontsize= 15)\n\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:04.794910Z","iopub.execute_input":"2022-04-06T14:02:04.795190Z","iopub.status.idle":"2022-04-06T14:02:07.491518Z","shell.execute_reply.started":"2022-04-06T14:02:04.795133Z","shell.execute_reply":"2022-04-06T14:02:07.490607Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# ***Scalling Data***","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:07.492902Z","iopub.execute_input":"2022-04-06T14:02:07.493212Z","iopub.status.idle":"2022-04-06T14:02:07.497392Z","shell.execute_reply.started":"2022-04-06T14:02:07.493168Z","shell.execute_reply":"2022-04-06T14:02:07.496766Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"#using RobustScaler to scale de time and amount columns\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Amount','Time'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:07.500626Z","iopub.execute_input":"2022-04-06T14:02:07.501090Z","iopub.status.idle":"2022-04-06T14:02:07.646627Z","shell.execute_reply.started":"2022-04-06T14:02:07.501042Z","shell.execute_reply":"2022-04-06T14:02:07.645651Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:07.647933Z","iopub.execute_input":"2022-04-06T14:02:07.648231Z","iopub.status.idle":"2022-04-06T14:02:07.676284Z","shell.execute_reply.started":"2022-04-06T14:02:07.648179Z","shell.execute_reply":"2022-04-06T14:02:07.675341Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# putting scaled amount and time in the beginning (optional)\n\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount','scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_time', scaled_time)\ndf.insert(1, 'scaled_amount', scaled_amount)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:07.677780Z","iopub.execute_input":"2022-04-06T14:02:07.678886Z","iopub.status.idle":"2022-04-06T14:02:07.787084Z","shell.execute_reply.started":"2022-04-06T14:02:07.678798Z","shell.execute_reply":"2022-04-06T14:02:07.785976Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# ***Spliting dataset from training***","metadata":{}},{"cell_type":"markdown","source":"doing the split in this phase, makes better predict with all dataset and avoid overfitting or underfitting data sets created instead using the functions.\n\nWe will make a subdataset with class balanced 50/50","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nX = df.drop('Class',axis=1)\ny = df['Class']\n\nsss = StratifiedShuffleSplit(n_splits=5, random_state=None)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train\", train_index, \"Test\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n    \noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:07.788454Z","iopub.execute_input":"2022-04-06T14:02:07.788883Z","iopub.status.idle":"2022-04-06T14:02:08.775171Z","shell.execute_reply.started":"2022-04-06T14:02:07.788849Z","shell.execute_reply":"2022-04-06T14:02:08.773546Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"# Performing Random Undersampling \n* We balance dataset getting 50/50 distribution over our class for better fitting our model. \n* The risk of performing this technique involve shrink our non-defaulters from 284,315 register to 492 equaling the class but can loss some accuracy in our model, because information loss. ","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=1)\n\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnorm_dist_df = pd.concat([fraud_df, non_fraud_df])\n\nnew_df = norm_dist_df.sample(frac=1, random_state=42)\n\nnew_df","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:08.776705Z","iopub.execute_input":"2022-04-06T14:02:08.778364Z","iopub.status.idle":"2022-04-06T14:02:08.943757Z","shell.execute_reply.started":"2022-04-06T14:02:08.778304Z","shell.execute_reply":"2022-04-06T14:02:08.942849Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"print(new_df['Class'].value_counts()/len(new_df))\n\n\nsns.countplot('Class', data=new_df);","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:08.945503Z","iopub.execute_input":"2022-04-06T14:02:08.946048Z","iopub.status.idle":"2022-04-06T14:02:09.124656Z","shell.execute_reply.started":"2022-04-06T14:02:08.946000Z","shell.execute_reply":"2022-04-06T14:02:09.123935Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(22,20))\n\n\ncorr = df.corr()\nsns.heatmap(corr,cmap='coolwarm', annot_kws={'size':20}, ax=ax1)\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr,cmap='coolwarm', annot_kws={'size':20}, ax=ax2)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:09.125876Z","iopub.execute_input":"2022-04-06T14:02:09.126463Z","iopub.status.idle":"2022-04-06T14:02:11.648380Z","shell.execute_reply.started":"2022-04-06T14:02:09.126425Z","shell.execute_reply":"2022-04-06T14:02:11.647300Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# Considerations\n\n* Highly Positive correlated with class = V19, V11, V4, V2, when those variables are high, best best chances to predict fraud.\n\n* Highly Negative correlated with class = V17, V14, V12, V10, when feature values are low, have better chance to predict fraud.","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive Correlations with our Class (The higher our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, ax=axes[0])\naxes[0].set_title('V19 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, ax=axes[1])\naxes[1].set_title('V11 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, ax=axes[2])\naxes[2].set_title('V4 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, ax=axes[3])\naxes[3].set_title('V2 vs Class Negative Correlation')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:11.650038Z","iopub.execute_input":"2022-04-06T14:02:11.650527Z","iopub.status.idle":"2022-04-06T14:02:12.204460Z","shell.execute_reply.started":"2022-04-06T14:02:11.650483Z","shell.execute_reply":"2022-04-06T14:02:12.203322Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:12.205936Z","iopub.execute_input":"2022-04-06T14:02:12.206220Z","iopub.status.idle":"2022-04-06T14:02:12.726146Z","shell.execute_reply.started":"2022-04-06T14:02:12.206177Z","shell.execute_reply":"2022-04-06T14:02:12.724899Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Anomaly Detection","metadata":{}},{"cell_type":"markdown","source":"***Removing extreme outliers from features high correlated with our class will have a good impact on ours model accuracy.***\n* We use Interquartile Range (IQR) to calculate a new threshold of our features that have extreme outliers, we don't remove all outliers because of loss information and risk of underfiting our model.\n* Will be used some multiplier with the IQR to improve our threshold range, the lower multiplier used, more outliers will be removed. That was tunned to a better fit.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,8))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class']==1].values\nsns.distplot(v14_fraud_dist, ax=ax1, fit=norm)\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=12)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class']==1].values\nsns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='r')\nax1.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=12)\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class']==1].values\nsns.distplot(v10_fraud_dist, ax=ax3, fit=norm, color='g')\nax1.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:12.727862Z","iopub.execute_input":"2022-04-06T14:02:12.728106Z","iopub.status.idle":"2022-04-06T14:02:13.330881Z","shell.execute_reply.started":"2022-04-06T14:02:12.728078Z","shell.execute_reply":"2022-04-06T14:02:13.329924Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"## Removing outliers V14 (Highest Negative correlated with labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\n# 1.5 here is the multiplier for exclude outliers\nv14_cut_off = v14_iqr * 1.5 \nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off \nprint('Cut Off:{}'.format(v14_cut_off))\nprint('V14 Lower:{}'.format(v14_lower))\nprint('V14 Upper:{}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 25)\n\n## REPEAT PROCESS THROUGH MOST CORRELATED VARIABLES\n\n# V12 Feature\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5 \nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off \nprint('V12 Lower:{}'.format(v12_lower))\nprint('V12 Upper:{}'.format(v12_upper))\n\noutliers = [x for x in v14_fraud if x < v12_lower or x > v12_upper]\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V12 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 25)\n\n# V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5 \nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off \nprint('V10 Lower:{}'.format(v10_lower))\nprint('V10 Upper:{}'.format(v10_upper))\n\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of instances after outliers removal: {}'.format(len(new_df)))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:13.332555Z","iopub.execute_input":"2022-04-06T14:02:13.332811Z","iopub.status.idle":"2022-04-06T14:02:13.370651Z","shell.execute_reply.started":"2022-04-06T14:02:13.332779Z","shell.execute_reply":"2022-04-06T14:02:13.369646Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(18,5))\n\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=ax1, palette='pastel')\nax1.set_title(\"V14 Feature \\n Reduction of outliers\")\nax1.annotate('Less Extreme \\n outliers', xy=(0.98, -17.5), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\n# Feature V12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette='pastel')\nax2.set_title(\"V12 Feature \\n Reduction of outliers\")\nax2.annotate('Less Extreme \\n outliers', xy=(0.98, -17.3), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette='pastel')\nax3.set_title(\"V10 Feature \\n Reduction of outliers\")\nax3.annotate(\"Less Extreme \\n outliers\", xy=(0.98, -15.2), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:13.371994Z","iopub.execute_input":"2022-04-06T14:02:13.372344Z","iopub.status.idle":"2022-04-06T14:02:13.908847Z","shell.execute_reply.started":"2022-04-06T14:02:13.372295Z","shell.execute_reply":"2022-04-06T14:02:13.908220Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Reducing Dimension and Cluestering\n* We use t-SNE to reduce dimension and cluster or data, having small dataset may benefit us from this type of reduction improving accuracy.\n* Tryied 3 techniques to reduce dimensionality: t-SNE, PCA and Truncated SVD","metadata":{}},{"cell_type":"code","source":"\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# t-SNE implementation -- non-linear method\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"t-SNE took: {:.2} s\".format(t1-t0))\n\n# PCA Implementation -- linear method\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took: {:.2} s\".format(t1-t0))\n\n# Truncated SVD -- linear method\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"TruncatedSVD took: {:.2} s\".format(t1-t0))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:13.910688Z","iopub.execute_input":"2022-04-06T14:02:13.911039Z","iopub.status.idle":"2022-04-06T14:02:18.404269Z","shell.execute_reply.started":"2022-04-06T14:02:13.910993Z","shell.execute_reply":"2022-04-06T14:02:18.400684Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# seeing the results\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\nlabels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters Using Dimensionality Reduction', fontsize=14)\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label=labels[0])\nred_patch = mpatches.Patch(color='#AF0000', label=labels[1])\n\n\n# t-sne\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label=labels[0])\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label=labels[1])\nax1.set_title('t-SNE')\nax1.grid(True)\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA \nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label=labels[0])\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label=labels[1])\nax2.set_title('PCA')\nax2.grid(True)\nax2.legend(handles=[blue_patch, red_patch])\n\n# Truncated SVD\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label=labels[0])\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label=labels[1])\nax3.set_title('Truncated SVD')\nax3.grid(True)\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:18.406298Z","iopub.execute_input":"2022-04-06T14:02:18.406834Z","iopub.status.idle":"2022-04-06T14:02:19.167294Z","shell.execute_reply.started":"2022-04-06T14:02:18.406785Z","shell.execute_reply":"2022-04-06T14:02:19.166192Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"# Classifiers (UnderSampling)\n* In this section we perform 4 types of classifiers models and measure the Receiving Operating Characteristic (ROC), knowing the best model to our data.\n* We perform the train-test split on dataset.\n* Further more we do a Oversampling to analyse data too.\n* GridSearchCV establish the parameters with best predictive scores for those models.","metadata":{}},{"cell_type":"markdown","source":"***Undersampling before cross validating (prone to overfit)***","metadata":{}},{"cell_type":"code","source":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Explicitly used for undersampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n# models implementation\n\nclassifiers = {\n    \"LogisticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n    \n}\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:19.169009Z","iopub.execute_input":"2022-04-06T14:02:19.169871Z","iopub.status.idle":"2022-04-06T14:02:19.182568Z","shell.execute_reply.started":"2022-04-06T14:02:19.169819Z","shell.execute_reply":"2022-04-06T14:02:19.181874Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# scores getting higher even when applying Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2)* 100, \"%accuracy score\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:19.183948Z","iopub.execute_input":"2022-04-06T14:02:19.184304Z","iopub.status.idle":"2022-04-06T14:02:19.720232Z","shell.execute_reply.started":"2022-04-06T14:02:19.184258Z","shell.execute_reply":"2022-04-06T14:02:19.719574Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"## Testing models with random parameters looking for best fitting our model\n## Each model recieve distinct types of parameters, this is why a list is passed for each of them. \n\n\n# Using GridSearchCV \nfrom sklearn.model_selection import GridSearchCV\n\n# Logistic Regression parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# Automatically get the best logistic regression with best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n\n# KNearest Neighbors \nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\nknears = grid_knears.best_estimator_\n\n\n# SVC\nsvc_params = {\"C\": [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\nsvc = grid_svc.best_estimator_\n\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree = grid_tree.best_estimator_\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:19.721241Z","iopub.execute_input":"2022-04-06T14:02:19.721880Z","iopub.status.idle":"2022-04-06T14:02:23.393394Z","shell.execute_reply.started":"2022-04-06T14:02:19.721846Z","shell.execute_reply":"2022-04-06T14:02:23.392265Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:23.394650Z","iopub.execute_input":"2022-04-06T14:02:23.394893Z","iopub.status.idle":"2022-04-06T14:02:23.960357Z","shell.execute_reply.started":"2022-04-06T14:02:23.394864Z","shell.execute_reply":"2022-04-06T14:02:23.959287Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"***Undersampling during cross validating***","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom imblearn.under_sampling import RandomUnderSampler","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:23.964846Z","iopub.execute_input":"2022-04-06T14:02:23.965104Z","iopub.status.idle":"2022-04-06T14:02:23.972040Z","shell.execute_reply.started":"2022-04-06T14:02:23.965076Z","shell.execute_reply":"2022-04-06T14:02:23.970969Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"undersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values\n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n## Implementing NearMiss \nX_nearmiss, y_nearmiss = NearMiss().fit_resample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n### Cross Validating (the right way)\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n     \n    ### smote happen during cross validation not before..\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg)  \n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:23.973231Z","iopub.execute_input":"2022-04-06T14:02:23.973695Z","iopub.status.idle":"2022-04-06T14:02:41.343974Z","shell.execute_reply.started":"2022-04-06T14:02:23.973660Z","shell.execute_reply":"2022-04-06T14:02:41.342861Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:41.346021Z","iopub.execute_input":"2022-04-06T14:02:41.346717Z","iopub.status.idle":"2022-04-06T14:02:41.397220Z","shell.execute_reply.started":"2022-04-06T14:02:41.346667Z","shell.execute_reply":"2022-04-06T14:02:41.395737Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears, svc, tree, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:02:41.399948Z","iopub.execute_input":"2022-04-06T14:02:41.400682Z","iopub.status.idle":"2022-04-06T14:03:02.370501Z","shell.execute_reply.started":"2022-04-06T14:02:41.400634Z","shell.execute_reply":"2022-04-06T14:03:02.369444Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:02.371776Z","iopub.execute_input":"2022-04-06T14:03:02.372327Z","iopub.status.idle":"2022-04-06T14:03:02.376747Z","shell.execute_reply.started":"2022-04-06T14:03:02.372285Z","shell.execute_reply":"2022-04-06T14:03:02.376109Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method='decision_function' )\nknears_pred = cross_val_predict(knears, X_train, y_train, cv=5)\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method='decision_function')\ntree_pred = cross_val_predict(tree, X_train, y_train, cv=5)\n\nprint('Logistic regression : ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighboor : ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:02.377831Z","iopub.execute_input":"2022-04-06T14:03:02.378449Z","iopub.status.idle":"2022-04-06T14:03:02.953366Z","shell.execute_reply.started":"2022-04-06T14:03:02.378411Z","shell.execute_reply":"2022-04-06T14:03:02.952365Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='black', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:02.954851Z","iopub.execute_input":"2022-04-06T14:03:02.955450Z","iopub.status.idle":"2022-04-06T14:03:03.307502Z","shell.execute_reply.started":"2022-04-06T14:03:02.955391Z","shell.execute_reply":"2022-04-06T14:03:03.306408Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"## Terms:\n\n*     **True Positives**: Correctly Classified Fraud Transactions\n*     **False Positives**: Incorrectly Classified Fraud Transactions\n*     **True Negative**: Correctly Classified Non-Fraud Transactions\n*     **False Negative**: Incorrectly Classified Non-Fraud Transactions\n*     **Precision**: True Positives/(True Positives + False Positives)\n*     **Recall**: True Positives/(True Positives + False Negatives)\n*     Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.\n*     **Precision/Recall Tradeoff**: The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect.\n","metadata":{}},{"cell_type":"code","source":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:03.308861Z","iopub.execute_input":"2022-04-06T14:03:03.309117Z","iopub.status.idle":"2022-04-06T14:03:03.528999Z","shell.execute_reply.started":"2022-04-06T14:03:03.309086Z","shell.execute_reply":"2022-04-06T14:03:03.527972Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:03.530706Z","iopub.execute_input":"2022-04-06T14:03:03.531547Z","iopub.status.idle":"2022-04-06T14:03:03.558109Z","shell.execute_reply.started":"2022-04-06T14:03:03.531495Z","shell.execute_reply":"2022-04-06T14:03:03.557180Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"***The results above are overfited, because of the wrong cross validation process*** \n\n* this can be seen on average precision/recall curve graph.","metadata":{}},{"cell_type":"code","source":"### Build oversampling with SMOTE function and using cross validation during the process\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\nprint('Length of X (TRAIN): {} | Length of y (TRAIN): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (TEST): {} | Length of y (TEST): {}'.format(len(original_Xtest), len(original_ytest)))\n\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\nlog_reg_sm = LogisticRegression()\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n# Implementing SMOTE and Cross Validation (right way)\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) ## smote during CV, not before.\n    \n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:03:03.559791Z","iopub.execute_input":"2022-04-06T14:03:03.560317Z","iopub.status.idle":"2022-04-06T14:07:10.404058Z","shell.execute_reply.started":"2022-04-06T14:03:03.560270Z","shell.execute_reply":"2022-04-06T14:07:10.400812Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:07:10.405851Z","iopub.execute_input":"2022-04-06T14:07:10.406202Z","iopub.status.idle":"2022-04-06T14:07:10.486578Z","shell.execute_reply.started":"2022-04-06T14:07:10.406154Z","shell.execute_reply":"2022-04-06T14:07:10.483484Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"y_score = best_est.decision_function(original_Xtest)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:07:10.488379Z","iopub.execute_input":"2022-04-06T14:07:10.488695Z","iopub.status.idle":"2022-04-06T14:07:10.499892Z","shell.execute_reply.started":"2022-04-06T14:07:10.488653Z","shell.execute_reply":"2022-04-06T14:07:10.498817Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(original_ytest, y_score)\n\nprint('Average Precision Score: {0:0.3f}'.format(average_precision))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:07:10.501817Z","iopub.execute_input":"2022-04-06T14:07:10.502503Z","iopub.status.idle":"2022-04-06T14:07:10.522304Z","shell.execute_reply.started":"2022-04-06T14:07:10.502453Z","shell.execute_reply":"2022-04-06T14:07:10.521017Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nfig = plt.figure(figsize=(14,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='b', alpha=0.3, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nprint(\"Precision-Recall Curve with: AUC = {0:0.3f}\".format(average_precision))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:12:23.313830Z","iopub.execute_input":"2022-04-06T14:12:23.314182Z","iopub.status.idle":"2022-04-06T14:12:23.507202Z","shell.execute_reply.started":"2022-04-06T14:12:23.314126Z","shell.execute_reply":"2022-04-06T14:12:23.506099Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"# SMOTE after splitting and cross validating\nsm = SMOTE(sampling_strategy='minority', random_state=42)\n\nXsm_train, ysm_train = sm.fit_resample(original_Xtrain, original_ytrain)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:19:31.056870Z","iopub.execute_input":"2022-04-06T14:19:31.057210Z","iopub.status.idle":"2022-04-06T14:19:31.519745Z","shell.execute_reply.started":"2022-04-06T14:19:31.057174Z","shell.execute_reply":"2022-04-06T14:19:31.518850Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"t0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample took :{} sec\".format(t1-t0))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:20:37.644234Z","iopub.execute_input":"2022-04-06T14:20:37.644556Z","iopub.status.idle":"2022-04-06T14:20:47.229298Z","shell.execute_reply.started":"2022-04-06T14:20:37.644525Z","shell.execute_reply":"2022-04-06T14:20:47.228189Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"# TEST DATA WITH LOGISTIC REGRESSION\n","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\n#logistic regression with SMOTE technique\ny_pred_log_reg = log_reg_sm.predict(X_test)\n\n#other models fitted with UnderSampling\ny_pred_knear = knears.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree.predict(X_test)\n\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nfig, ax = plt.subplots(2, 2, figsize=(22,12))\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\")\nax[0, 0].set_xticklabels(['', ''], rotation=90)\nax[0, 0].set_yticklabels([\"\", \"\"], rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0, 1].set_title(\"KNearsNeighbors \\n Confusion Matrix\")\nax[0, 1].set_xticklabels(['', ''], rotation=90)\nax[0, 1].set_yticklabels([\"\", \"\"], rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1, 0].set_title(\"Support Vector Classifier \\n Confusion Matrix\")\nax[1, 0].set_xticklabels(['', ''], rotation=90)\nax[1, 0].set_yticklabels([\"\", \"\"], rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1, 1].set_title(\"Decision Tree Classifier \\n Confusion Matrix\")\nax[1, 1].set_xticklabels(['', ''], rotation=90)\nax[1, 1].set_yticklabels([\"\", \"\"], rotation=360)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:29:25.436090Z","iopub.execute_input":"2022-04-06T14:29:25.436465Z","iopub.status.idle":"2022-04-06T14:29:27.020471Z","shell.execute_reply.started":"2022-04-06T14:29:25.436428Z","shell.execute_reply":"2022-04-06T14:29:27.019388Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:30:36.937904Z","iopub.execute_input":"2022-04-06T14:30:36.938263Z","iopub.status.idle":"2022-04-06T14:30:36.965244Z","shell.execute_reply.started":"2022-04-06T14:30:36.938226Z","shell.execute_reply":"2022-04-06T14:30:36.964559Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n\n\n# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n\ng = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=g)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n# Note how high is accuracy score it can be misleading! \nfinal_df","metadata":{"execution":{"iopub.status.busy":"2022-04-06T14:34:15.624708Z","iopub.execute_input":"2022-04-06T14:34:15.625384Z","iopub.status.idle":"2022-04-06T14:34:15.661154Z","shell.execute_reply.started":"2022-04-06T14:34:15.625329Z","shell.execute_reply":"2022-04-06T14:34:15.659775Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# Building Neural Network with Keras","metadata":{}},{"cell_type":"code","source":"import keras\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras import layers\nfrom keras.metrics import categorical_crossentropy\n\n\nn_inputs = X_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:25:48.501121Z","iopub.execute_input":"2022-04-06T15:25:48.501926Z","iopub.status.idle":"2022-04-06T15:25:48.509369Z","shell.execute_reply.started":"2022-04-06T15:25:48.501872Z","shell.execute_reply":"2022-04-06T15:25:48.508587Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"markdown","source":"***FIRST MODEL WILL RECIEVE UNDERSAMPLE***","metadata":{}},{"cell_type":"code","source":"undersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:25:51.328772Z","iopub.execute_input":"2022-04-06T15:25:51.329264Z","iopub.status.idle":"2022-04-06T15:25:51.365581Z","shell.execute_reply.started":"2022-04-06T15:25:51.329218Z","shell.execute_reply":"2022-04-06T15:25:51.364789Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"undersample_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:25:51.967783Z","iopub.execute_input":"2022-04-06T15:25:51.968461Z","iopub.status.idle":"2022-04-06T15:25:51.977045Z","shell.execute_reply.started":"2022-04-06T15:25:51.968405Z","shell.execute_reply":"2022-04-06T15:25:51.976016Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:05.226222Z","iopub.execute_input":"2022-04-06T15:26:05.227101Z","iopub.status.idle":"2022-04-06T15:26:05.238535Z","shell.execute_reply.started":"2022-04-06T15:26:05.227058Z","shell.execute_reply":"2022-04-06T15:26:05.237377Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:06.233852Z","iopub.execute_input":"2022-04-06T15:26:06.234162Z","iopub.status.idle":"2022-04-06T15:26:09.262212Z","shell.execute_reply.started":"2022-04-06T15:26:06.234117Z","shell.execute_reply":"2022-04-06T15:26:09.261241Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:11.442003Z","iopub.execute_input":"2022-04-06T15:26:11.442359Z","iopub.status.idle":"2022-04-06T15:26:11.856525Z","shell.execute_reply.started":"2022-04-06T15:26:11.442322Z","shell.execute_reply":"2022-04-06T15:26:11.855658Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"undersample_fraud_predictions = np.argmax(undersample_predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:13.652531Z","iopub.execute_input":"2022-04-06T15:26:13.652845Z","iopub.status.idle":"2022-04-06T15:26:13.657927Z","shell.execute_reply.started":"2022-04-06T15:26:13.652814Z","shell.execute_reply":"2022-04-06T15:26:13.656838Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"undersample_fraud_predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:14.348319Z","iopub.execute_input":"2022-04-06T15:26:14.348628Z","iopub.status.idle":"2022-04-06T15:26:14.355204Z","shell.execute_reply.started":"2022-04-06T15:26:14.348598Z","shell.execute_reply":"2022-04-06T15:26:14.354098Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:17.872789Z","iopub.execute_input":"2022-04-06T15:26:17.873273Z","iopub.status.idle":"2022-04-06T15:26:17.884788Z","shell.execute_reply.started":"2022-04-06T15:26:17.873235Z","shell.execute_reply":"2022-04-06T15:26:17.883982Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:23.144201Z","iopub.execute_input":"2022-04-06T15:26:23.144718Z","iopub.status.idle":"2022-04-06T15:26:23.649576Z","shell.execute_reply.started":"2022-04-06T15:26:23.144667Z","shell.execute_reply":"2022-04-06T15:26:23.648626Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"markdown","source":"***SECOND MODEL WILL RECIEVE OVERSAMPLE DATASET(SMOTE):***","metadata":{}},{"cell_type":"code","source":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:35.891879Z","iopub.execute_input":"2022-04-06T15:26:35.892223Z","iopub.status.idle":"2022-04-06T15:26:35.927343Z","shell.execute_reply.started":"2022-04-06T15:26:35.892185Z","shell.execute_reply":"2022-04-06T15:26:35.926293Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:26:36.727832Z","iopub.execute_input":"2022-04-06T15:26:36.728127Z","iopub.status.idle":"2022-04-06T15:26:36.739473Z","shell.execute_reply.started":"2022-04-06T15:26:36.728097Z","shell.execute_reply":"2022-04-06T15:26:36.738550Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:27:50.373952Z","iopub.execute_input":"2022-04-06T15:27:50.374877Z","iopub.status.idle":"2022-04-06T15:29:06.695281Z","shell.execute_reply.started":"2022-04-06T15:27:50.374821Z","shell.execute_reply":"2022-04-06T15:29:06.694238Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:29:19.133883Z","iopub.execute_input":"2022-04-06T15:29:19.134230Z","iopub.status.idle":"2022-04-06T15:29:19.423836Z","shell.execute_reply.started":"2022-04-06T15:29:19.134192Z","shell.execute_reply":"2022-04-06T15:29:19.422823Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"oversample_fraud_predictions = np.argmax(oversample_predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:30:38.989590Z","iopub.execute_input":"2022-04-06T15:30:38.989908Z","iopub.status.idle":"2022-04-06T15:30:38.995520Z","shell.execute_reply.started":"2022-04-06T15:30:38.989874Z","shell.execute_reply":"2022-04-06T15:30:38.994382Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(18,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"Oversample (Smote) \\n Confusion Matrix\")\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title='Confusion Matrix \\n (with 100% accuracy)', cmap=plt.cm.Oranges)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:34:05.153918Z","iopub.execute_input":"2022-04-06T15:34:05.154877Z","iopub.status.idle":"2022-04-06T15:34:05.589458Z","shell.execute_reply.started":"2022-04-06T15:34:05.154823Z","shell.execute_reply":"2022-04-06T15:34:05.588629Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* In this project we can see that our model Oversampled got more False Positives than with Undersample.\n* Some points to remember are that Smote function was used on original dataset without removing outliers that could affect our accuracy.\n* We need to consider from a business perspective that False Negatives could be annoying for our clients, implying cards blocked with a normal transactions, this could hurt companny stability with consumers.","metadata":{}}]}